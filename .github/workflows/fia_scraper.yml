name: FIA Scraper

on:
  schedule:
    - cron: '*/5 * * * 5'  # Friday
    - cron: '*/5 * * * 6'  # Saturday
    - cron: '*/5 * * * 0'  # Sunday
    - cron: '0 1 * * 1'    # Monday
    - cron: '0 1 * * 2'    # Tuesday
    - cron: '0 1 * * 3'    # Wednesday
    - cron: '0 1 * * 4'    # Thursday
  workflow_dispatch:
    inputs:
      force:
        description: "Force run regardless of race weekend"
        required: false
        default: "false"
  push:
    paths:
      - "fia_scraper/**"
      - ".github/workflows/fia_scraper.yml"

jobs:
  scrape-and-notify:
    runs-on: ubuntu-24.04

    env:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      DISCORD_ERROR_WEBHOOK_URL: ${{ secrets.DISCORD_ERROR_WEBHOOK_URL }}

    steps:
      - name: ‚úÖ Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: 3.12

      - name: üì¶ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: üîÅ Restore cached document hashes
        id: cache-restore
        uses: actions/cache@v4
        with:
          path: last_fia_doc_hash.txt
          key: fia-doc-cache-v2
          restore-keys: fia-doc-cache-v2

      - name: üß† Run FIA scraper
        run: |
          xvfb-run --auto-servernum python fia_scraper/scraper.py ${{ inputs.force == 'true' && '--force' || '' }}

      - name: üíæ Save updated cache (only if not restored)
        if: steps.cache-restore.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: last_fia_doc_hash.txt
          key: fia-doc-cache-v2-${{ github.run_id }}
